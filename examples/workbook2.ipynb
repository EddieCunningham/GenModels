{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd\n",
    "import autograd.numpy as anp\n",
    "import itertools\n",
    "from functools import partial, namedtuple\n",
    "from autograd.extend import primitive, defvjp\n",
    "from autograd import jacobian as jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp( v, axis=0 ):\n",
    "    max_v = anp.max( v )\n",
    "    return anp.log( anp.sum( anp.exp( v - max_v ), axis=axis ) ) + max_v\n",
    "\n",
    "def logsumexp_vjp(ans, x):\n",
    "    x_shape = x.shape\n",
    "    return lambda g: anp.full(x_shape, g) * anp.exp(x - np.full(x_shape, ans))\n",
    "defvjp(logsumexp, logsumexp_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbelSample( shape, eps=1e-8 ):\n",
    "    u = anp.random.random( shape )\n",
    "    return -anp.log( -anp.log( u + eps ) + eps )\n",
    "def gumbelSoftmaxSample( logits, g=None, temp=1.0 ):\n",
    "    if( g is None ):\n",
    "        g = gumbelSample( logits.shape )\n",
    "    y = logits + g\n",
    "    ans = anp.exp( y ) / temp\n",
    "    return ans / ans.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphas_unrolled( theta ):\n",
    "    T, K = theta.L.shape\n",
    "    a_0 = theta.pi0 + theta.L[ 0 ]\n",
    "    a_1 = logsumexp( a_0[ :, None ] + theta.pi, axis=0 ) + theta.L[ 1 ]\n",
    "    a_2 = logsumexp( a_1[ :, None ] + theta.pi, axis=0 ) + theta.L[ 2 ]\n",
    "    return anp.array( [ a_0, a_1, a_2 ] )\n",
    "\n",
    "def betas_unrolled( theta ):\n",
    "    T, K = theta.L.shape\n",
    "    b_2 = anp.zeros( K )\n",
    "    b_1 = logsumexp( b_2 + theta.pi + theta.L[ 2 ], axis=1 )\n",
    "    b_0 = logsumexp( b_1 + theta.pi + theta.L[ 1 ], axis=1 )\n",
    "    return anp.array( [ b_0, b_1, b_2 ] )    \n",
    "\n",
    "def joints_unrolled( alpha, beta ):\n",
    "    j_1 = alpha[ 0 ][ :, None ] + pi + L[ 1 ] + beta[ 1 ]\n",
    "    j_2 = alpha[ 1 ][ :, None ] + pi + L[ 2 ] + beta[ 2 ]\n",
    "    return anp.array( [ j_1, j_2 ] )\n",
    "\n",
    "def predictive_unrolled( alpha, beta ):\n",
    "    T, d_latent = alpha.shape\n",
    "    joint = joints_unrolled( alpha, beta )\n",
    "    return joint - anp.reshape( ( alpha + beta )[ :-1 ], ( T-1, d_latent, 1 ) )\n",
    "    \n",
    "def alphas( theta ):\n",
    "    T, K = theta.L.shape\n",
    "    alpha = anp.zeros( ( T, K ) )\n",
    "    alpha[ 0 ] = theta.pi0 + theta.L[ 0 ]\n",
    "    for t in range( 1, T ):\n",
    "        alpha[ t ] = logsumexp( alpha[ t - 1 ][ :, None ] + theta.pi, axis=0 ) + theta.L[ t ]\n",
    "    return alpha\n",
    "\n",
    "def betas( theta ):\n",
    "    T, K = theta.L.shape\n",
    "    beta = anp.zeros( ( T, K ) )\n",
    "    for t in reversed( range( 0, T - 1 ) ):\n",
    "        beta[ t ] = logsumexp( beta[ t + 1 ] + theta.pi + theta.L[ t + 1 ], axis=1 )\n",
    "    return beta\n",
    "\n",
    "def joints( alpha, beta ):\n",
    "    joints = anp.zeros( ( T-1, d_latent, d_latent ) )\n",
    "    for t in range( T - 1 ):\n",
    "        joints[ t ] = alpha[ t ][ :, None ] + pi + L[ t + 1 ] + beta[ t + 1 ]\n",
    "    return joints\n",
    "\n",
    "def predictive( alpha, beta ):\n",
    "    T, d_latent = alpha.shape\n",
    "    joint = joints( alpha, beta )\n",
    "    return joint - anp.reshape( ( alpha + beta )[ :-1 ], ( T-1, d_latent, 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleX( alpha, beta ):\n",
    "    T, K = alpha.shape\n",
    "    log_z = logsumexp( alpha[ -1 ] )\n",
    "    \n",
    "    preds = predictive( alpha, beta )\n",
    "    x_samples = anp.zeros( ( T, K ) )\n",
    "    \n",
    "    x_samples[ 0 ] = alpha[ 0 ] + beta[ 0 ] - log_z\n",
    "    \n",
    "    for t, p in enumerate( preds ):\n",
    "        logits = logsumexp( p + x_samples[ t ], axis=1 )\n",
    "        x_samples[ t + 1 ] = anp.log( gumbelSoftmaxSample( logits ) )\n",
    "        \n",
    "    return x_samples\n",
    "\n",
    "def sampleX_unrolled( alpha, beta, gumb ):\n",
    "    T, K = alpha.shape\n",
    "    log_z = logsumexp( alpha[ -1 ] )\n",
    "        \n",
    "    preds = predictive_unrolled( alpha, beta )\n",
    "    \n",
    "    x_0 = alpha[ 0 ] + beta[ 0 ] - log_z\n",
    "    \n",
    "    logits = anp.log( anp.exp( preds[ 0 ] ) @ anp.exp( x_0 ) )\n",
    "    x_1 = anp.log( gumbelSoftmaxSample( logits, g=gumb[ 1 ] ) )\n",
    "    \n",
    "    logits = anp.log( anp.exp( preds[ 1 ] ) @ anp.exp( x_1 ) )\n",
    "    x_2 = anp.log( gumbelSoftmaxSample( logits, g=gumb[ 1 ] ) )\n",
    "    \n",
    "    return anp.array( [ x_0, x_1, x_2 ] )\n",
    "\n",
    "def hmmSamples( theta ):\n",
    "    alpha, beta = alphas( theta ), betas( theta )\n",
    "    return sampleX( alpha, beta )\n",
    "\n",
    "def hmmSamples_unrolled( theta, gumb ):\n",
    "    alpha, beta = alphas_unrolled( theta ), betas_unrolled( theta )\n",
    "    return sampleX_unrolled( alpha, beta, gumb )\n",
    "\n",
    "def neuralNet( x_samples, theta ):\n",
    "    \n",
    "    N = theta.d_latent * theta.d_obs\n",
    "    W = anp.arange( N ).reshape( ( theta.d_latent, theta.d_obs ) )\n",
    "    \n",
    "    y_dist = anp.einsum( 'ij,ti->tj', W, x_samples )\n",
    "    probs = y_dist[ anp.arange( theta.T ), theta.y ]\n",
    "    return anp.sum( probs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta = namedtuple( 'Theta', [ 'pi0', 'pi', 'L', 'T', 'd_latent', 'd_obs', 'y' ] )\n",
    "T = 3\n",
    "d_latent = 3\n",
    "d_obs = 2\n",
    "y = np.random.choice( d_obs, size=T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0 = np.random.random( d_latent )\n",
    "pi = np.random.random( ( d_latent, d_latent ) )\n",
    "pi0 = np.log( pi0 )\n",
    "pi = np.log( pi )\n",
    "L = np.random.random( ( d_latent, d_obs ) )\n",
    "L = L.T[ y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumb = gumbelSample( ( T, d_latent ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trueAnswer( L ):\n",
    "    theta = Theta( pi0, pi, L, T, d_latent, d_obs, y )\n",
    "    x_samples = hmmSamples_unrolled( theta, gumb )\n",
    "    return neuralNet( x_samples, theta )\n",
    "jac( trueAnswer )( L )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a( L ):\n",
    "    theta = Theta( pi0, pi, L, T, d_latent, d_obs, y )\n",
    "    x_samples = hmmSamples_unrolled( theta, gumb )\n",
    "    return x_samples\n",
    "def b( x_samples ):\n",
    "    theta = Theta( pi0, pi, L, T, d_latent, d_obs, y )\n",
    "    return neuralNet( x_samples, theta )\n",
    "da = jac( a )( L )\n",
    "db = jac( b )( a( L ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum( 'ijab,ij->ab', da, db )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Compute x samples\n",
    "## Step 2 - Compute dlogP( y | x )/dx\n",
    "## Step 3 - Accumulate dlogP( y | x )/dL by computing dx/dL and summing immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv( theta, gumb ):\n",
    "    temp = 1.0\n",
    "    \n",
    "    # Get the needed stats\n",
    "    alpha, beta = alphas( theta ), betas( theta )\n",
    "    preds = predictive( alpha, beta )\n",
    "    log_z = logsumexp( alpha[ -1 ] )\n",
    "    \n",
    "    T, K = alpha.shape\n",
    "    \n",
    "    # Initialize the variables\n",
    "    dXtdLs = np.zeros( ( T, T, K, K ) )\n",
    "    dXtdXt1 = np.zeros( ( T-1, K, K ) )\n",
    "    x_samples = np.zeros( ( T, K ) )\n",
    "    \n",
    "    # Base case derivative\n",
    "    x_samples[ 0 ] = alpha[ 0 ] + beta[ 0 ] - log_z\n",
    "    dXtdLs[ 0, 0 ] = np.eye( K ) - np.exp( x_samples[ 0 ] )\n",
    "\n",
    "    print( '\\nt', 0, 's', 0, 'dXtdLs[ t, t ]\\n', dXtdLs[ 0, 0 ] )\n",
    "    \n",
    "    for i, p in enumerate( preds ):\n",
    "        t = i + 1\n",
    "        \n",
    "        # Compute x_t | x_t-1\n",
    "        p = theta.pi + theta.L[ t ] + beta[ t ] - beta[ t - 1 ][ :, None ]\n",
    "        logits = logsumexp( p + x_samples[ t - 1 ], axis=1 )\n",
    "        unnormx = logits + gumb[ t ] - np.log( temp )\n",
    "        x_samples[ t ] = unnormx - logsumexp( unnormx )\n",
    "        \n",
    "        # Compute dLogit / dL_t\n",
    "        dXPdLt = np.eye( K ) - np.exp( theta.L[ t ] + theta.pi + beta[ t ] ).T\n",
    "        \n",
    "        # Compute dLogit / dX_t-1\n",
    "        dLogitdXt1 = np.exp( p + x_samples[ t - 1 ] - logits[ :, None ] )\n",
    "        dLogitdP = dLogitdXt1\n",
    "        \n",
    "        # Compute dX_t / dLogit\n",
    "        dXtdLogit = np.eye( K ) - np.exp( x_samples[ t ] )\n",
    "        \n",
    "        # Compute dX_t / dX_t-1\n",
    "        dXtdXt1[ i ] = dXtdLogit @ dLogitdXt1\n",
    "    \n",
    "        # Compute the derivative dX_t / dL_s for s == t\n",
    "        dXtdLs[ t, t ] = dXtdLogit @ dLogitdP @ dXPdLt\n",
    "        \n",
    "        # Update each of the L derivatives\n",
    "        for s in reversed( range( t ) ):\n",
    "            # Compute dX_t / dL_s for s < t\n",
    "            dXtdLs[ t, s ] = dXtdXt1[ i ] @ dXtdLs[ t-1, s ]\n",
    "        \n",
    "    return dXtdLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = Theta( pi0, pi, L, T, d_latent, d_obs, y )\n",
    "deriv( theta, gumb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.transpose( 0, 2, 1, 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = alphas( theta ), betas( theta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forAG( func ):\n",
    "    def wrapper( _L ):\n",
    "        theta = Theta( pi0, pi, _L, T, d_latent, d_obs, y )\n",
    "        return func( theta )\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_unrolled( theta ):\n",
    "    beta = betas_unrolled( theta )\n",
    "    p1 = theta.pi + theta.L[ 1 ] + beta[ 1 ] - beta[ 0 ][ :, None ]\n",
    "    p2 = theta.pi + theta.L[ 2 ] + beta[ 2 ] - beta[ 1 ][ :, None ]\n",
    "    return anp.array( [ p1, p2 ] )\n",
    "@forAG\n",
    "def pred_unrolled_ag( theta ):\n",
    "    return pred_unrolled( theta )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jac( pred_unrolled_ag )( L ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mypred_jac( theta ):\n",
    "    preds = pred_unrolled( theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d*beta*<sup>(t)</sup>/d*L*<sup>(s)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betas_unrolled( theta ):\n",
    "    T, K = theta.L.shape\n",
    "    b_2 = anp.zeros( K )\n",
    "    b_1 = logsumexp( b_2 + theta.pi + theta.L[ 2 ], axis=1 )\n",
    "    b_0 = logsumexp( b_1 + theta.pi + theta.L[ 1 ], axis=1 )\n",
    "    return anp.array( [ b_0, b_1, b_2 ] )    \n",
    "\n",
    "@forAG\n",
    "def betas_unrolled_ag( theta ):\n",
    "    return betas_unrolled( theta )\n",
    "\n",
    "def dbetadL( theta ):\n",
    "    betas = betas_unrolled( theta )\n",
    "    T, K = betas.shape\n",
    "    betas_jac = np.zeros( ( T, K, T, K ) )\n",
    "    for t in range( T-2, -1, -1 ):\n",
    "        val = np.exp( theta.pi + theta.L[ t+1 ] + betas[ t+1 ] - betas[ t ][ :, None ] )\n",
    "        betas_jac[ t, :, t+1, : ] = val\n",
    "        for s in range( t+2, T ):\n",
    "            betas_jac[ t, :, s, : ] = val @ betas_jac[ t+1, :, s, : ]\n",
    "    return betas_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d*F<sup>(t)</sup>*/d*L<sup>(s)</sup>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_unrolled( theta ):\n",
    "    beta = betas_unrolled( theta )\n",
    "    f1 = theta.pi + theta.L[ 1 ] + beta[ 1 ] - beta[ 0 ][ :, None ]\n",
    "    f2 = theta.pi + theta.L[ 2 ] + beta[ 2 ] - beta[ 1 ][ :, None ]\n",
    "    return anp.array( [ f1, f2 ] )\n",
    "\n",
    "@forAG\n",
    "def F_unrolled_ag( theta ):\n",
    "    return F_unrolled( theta )\n",
    "\n",
    "def dFdL( theta ):\n",
    "    F = F_unrolled( theta )\n",
    "    T, K = theta.L.shape\n",
    "    f_jac = np.zeros( F.shape + theta.L.shape )\n",
    "    b_jac = dbetadL( theta )\n",
    "    for t in range( 1, T ):\n",
    "        val = np.eye( K ) - np.exp( F[ t-1 ] )\n",
    "        for s in range( T ):\n",
    "            \n",
    "            f_jac[ t-1, :, :, s, : ] = b_jac[ t, :, s, : ] - b_jac[ t-1, :, s, : ][ :, None, : ]\n",
    "            if( t == s ):\n",
    "                f_jac[ t-1, :, :, s, : ] += np.eye( K )\n",
    "            \n",
    "    return f_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d*H<sup>(t)</sup>*/d*L<sup>(s)</sup>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_samples_unrolled( theta, gumb, temp=1.0 ):\n",
    "    F = F_unrolled( theta )\n",
    "    T, K = theta.T, theta.d_latent\n",
    "    x_samples = anp.zeros( ( T, K ) )\n",
    "    \n",
    "    alpha, beta = alphas_unrolled( theta ), betas_unrolled( theta )\n",
    "    log_z = logsumexp( alpha[ -1 ] )\n",
    "    x_samples_0 = theta.pi0 + theta.L[ 0 ] + beta[ 0 ] - log_z\n",
    "    \n",
    "    H0 = logsumexp( F[ 0 ] + x_samples_0, axis=1 )\n",
    "    G = H0 + gumb[ 1 ] - anp.log( temp )\n",
    "    x_samples_1 = G - logsumexp( G )\n",
    "\n",
    "    H1 = logsumexp( F[ 1 ] + x_samples_1, axis=1 )\n",
    "    G = H1 + gumb[ 2 ] - anp.log( temp )\n",
    "    x_samples_2 = G - logsumexp( G )\n",
    "\n",
    "    return anp.array( [ x_samples_0, x_samples_1, x_samples_2 ] )\n",
    "\n",
    "@forAG\n",
    "def x_samples_unrolled_ag( theta ):\n",
    "    return x_samples_unrolled( theta, gumb )\n",
    "\n",
    "def dHdL( theta, gumb, temp=1.0 ):\n",
    "    F = F_unrolled( theta )\n",
    "    dF = dFdL( theta )\n",
    "    dB = dbetadL( theta )\n",
    "    \n",
    "    T, K = theta.T, theta.d_latent\n",
    "    x_samples = anp.zeros( ( T, K ) )\n",
    "    dX = np.zeros( ( T, K, T, K ) )\n",
    "    \n",
    "    alpha, beta = alphas_unrolled( theta ), betas_unrolled( theta )\n",
    "    log_z = logsumexp( alpha[ -1 ] )\n",
    "    x_samples[ 0 ] = theta.pi0 + theta.L[ 0 ] + beta[ 0 ] - log_z\n",
    "    \n",
    "    dX[ 0 ] = dB[ 0 ]\n",
    "    dX[ 0, :, :, : ] -= np.exp( alpha + beta - log_z )\n",
    "    dX[ 0, :, 0, : ] += np.eye( K )\n",
    "    \n",
    "    for t in range( 1, T ):\n",
    "        H = logsumexp( F[ t-1 ] + x_samples[ t-1 ], axis=1 )\n",
    "        G = H + gumb[ t ] - anp.log( temp )\n",
    "        x_samples[ t ] = G - logsumexp( G )\n",
    "        \n",
    "        # H are the logits\n",
    "        # F are the conditioned transition matrix\n",
    "        # G are the unnormalized x samples\n",
    "\n",
    "        for s in range( T ):\n",
    "            deriv = dF[ t-1, :, :, s, : ] + dX[ t-1, :, s, : ][ None, :, : ]\n",
    "            val = np.exp( F[ t-1 ] + x_samples[ t-1 ] - H[ :, None ] )\n",
    "            dH = np.einsum( 'ij,ijk->ijk', val, deriv )\n",
    "            tmp = 1 - np.exp( G - logsumexp( G ) )\n",
    "            dX[ t, :, s, : ] = np.einsum( 'i,ijk->ik', tmp, dH )\n",
    "        \n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dHdL( theta, gumb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( x_samples_unrolled_ag )( L )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones( 4 )[ None, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbelSample( shape, eps=1e-8 ):\n",
    "    u = anp.random.random( shape )\n",
    "    return -anp.log( -anp.log( u + eps ) + eps )\n",
    "gumb = gumbelSample( ( T, d_latent ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation( x ):\n",
    "    return anp.sin( x )\n",
    "\n",
    "def check( x ):\n",
    "    d_in = x.shape[ -1 ]\n",
    "    d_out = 100\n",
    "    W1 = anp.arange( d_in * d_out ).reshape( ( d_in, d_out ) )\n",
    "    b1 = anp.arange( d_out )\n",
    "    \n",
    "    d_in = 100\n",
    "    d_out = 1\n",
    "    W2 = anp.arange( d_in * d_out ).reshape( ( d_in, d_out ) )\n",
    "    b2 = anp.arange( d_out )\n",
    "    \n",
    "    z = activation( anp.einsum( 'ij,ti->tj', W1, x ) + b1 )\n",
    "    \n",
    "    return anp.sum( activation( anp.einsum( 'ij,ti->tj', W2, z + b2 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random( ( 10, 3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "jac( check )( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asdf( x ):\n",
    "    return anp.sum( x**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( asdf )( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd( theta ):\n",
    "    T, K = theta.L.shape\n",
    "    alpha = []\n",
    "    alpha.append( theta.pi0 + theta.L[ 0 ] )\n",
    "    for t in range( 1, T ):\n",
    "        alpha.append( logsumexp( alpha[ -1 ][ :, None ] + theta.pi, axis=0 ) + theta.L[ t ] )\n",
    "    return anp.array( alpha )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@forAG\n",
    "def fwdAG( theta ):\n",
    "    return fwd( theta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( fwdAG )( L )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@forAG\n",
    "def blah( theta ):\n",
    "    return alphas_unrolled( theta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( blah )( L )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonFBSMultiplyTerms( terms ):\n",
    "    # Basically np.einsum but in log space\n",
    "    terms = list( terms )\n",
    "\n",
    "    # Separate out where the feedback set axes start and get the largest fbs_axis.\n",
    "    # Need to handle case where ndim of term > all fbs axes\n",
    "    # terms, fbs_axes_start = list( zip( *terms ) )\n",
    "    fbs_axes_start = [ -1, -1 ]\n",
    "\n",
    "    if( max( fbs_axes_start ) != -1 ):\n",
    "        max_fbs_axis = max( [ ax if ax != -1 else term.ndim for ax, term in zip( fbs_axes_start, terms ) ] )\n",
    "\n",
    "        if( max_fbs_axis > 0 ):\n",
    "            # Pad extra dims at each term so that the fbs axes start the same way for every term\n",
    "            for i, ax in enumerate( fbs_axes_start ):\n",
    "                if( ax == -1 ):\n",
    "                    for _ in range( max_fbs_axis - terms[ i ].ndim + 1 ):\n",
    "                        terms[ i ] = terms[ i ][ ..., None ]\n",
    "                else:\n",
    "                    for _ in range( max_fbs_axis - ax ):\n",
    "                        terms[ i ] = anp.expand_dims( terms[ i ], axis=ax )\n",
    "    else:\n",
    "        max_fbs_axis = -1\n",
    "\n",
    "    ndim = max( [ len( term.shape ) for term in terms ] )\n",
    "\n",
    "    axes = [ [ i for i, s in enumerate( t.shape ) if s != 1 ] for t in terms ]\n",
    "\n",
    "    # Get the shape of the output\n",
    "    shape = anp.ones( ndim, dtype=int )\n",
    "    for ax, term in zip( axes, terms ):\n",
    "        shape[ anp.array( ax ) ] = term.squeeze().shape\n",
    "\n",
    "    total_elts = shape.prod()\n",
    "    if( total_elts > 1e8 ):\n",
    "        assert 0, 'Don\\'t do this on a cpu!  Too many terms: %d'%( int( total_elts ) )\n",
    "\n",
    "    # Basically np.einsum in log space\n",
    "    ans = anp.zeros( shape )\n",
    "    for ax, term in zip( axes, terms ):\n",
    "\n",
    "        for _ in range( ndim - term.ndim ):\n",
    "            term = term[ ..., None ]\n",
    "\n",
    "        ans += term\n",
    "#         ans += np.broadcast_to( term, ans.shape )\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah( x ):\n",
    "    y = np.random.random( ( 1, 4, 2, 1, 1, 1, 8 ) )\n",
    "    return nonFBSMultiplyTerms( ( x, y ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random( ( 3, 1, 2, 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Something = recordclass.recordclass( 'something', [ 'a', 'b' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah2( s ):\n",
    "    s.a *= 4\n",
    "    return ( s.a * s.b )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blahh( a ):\n",
    "    s = Something( a, 5 )\n",
    "    return blah2( s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blahh( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( blahh )( 3.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah3( a ):\n",
    "    a *= 4\n",
    "    return ( a*5 )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( blah3 )( 3.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah3( 3.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tester():\n",
    "    def __init__( self ):\n",
    "        self.a = 4\n",
    "        \n",
    "    def blah( self, b ):\n",
    "        self.b = b\n",
    "        self.stuff()\n",
    "        return self.c\n",
    "        \n",
    "    def stuff( self ):\n",
    "        self.b *= 4\n",
    "        self.c = ( self.b * 3 )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.blah( 2.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah4( b ):\n",
    "    t = tester()\n",
    "    return t.blah( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah4( 2.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac( blah4 )( 2.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordclass import recordclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Succ = recordclass( 'Succ', [ 'a', 'b' ] )\n",
    "class blahh( Succ ):\n",
    "    @property\n",
    "    def yo( self ):\n",
    "        return self.a*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = blahh( 3, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah.yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.misc.optimizers import adam\n",
    "from autograd import grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah( params, iter ):\n",
    "    a, b, c = params\n",
    "    return np.sum( np.sum( a )*np.sum( b ) + c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random( ( 4, 3 ) )\n",
    "b = np.random.random( ( 4, 3 ) )\n",
    "c = np.random.random( ( 4, 3 ) )\n",
    "params = ( a, b, c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad( blah )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g( params, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah( a, b ):\n",
    "    print( a, b )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = partial( blah, b=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k( 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k( 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "letters = string.ascii_lowercase[ :10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters + ',' + ','.join( [ i for i in letters[ :-1 ] ] ) + '->' + letters[ -1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = None\n",
    "def blah( a ):\n",
    "    global t\n",
    "    b = np.sum( a, axis=0 )\n",
    "    def blah2( b ):\n",
    "        c = np.linalg.cholesky( b )\n",
    "        d = np.sum( c )\n",
    "        return d\n",
    "    val, t = value_and_grad( blah2 )( b )\n",
    "    return val\n",
    "\n",
    "def blahh( a ):\n",
    "    b = np.sum( a, axis=0 )\n",
    "    c = np.linalg.cholesky( b )\n",
    "    d = np.sum( c )\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "a = scipy.stats.invwishart.rvs( scale=np.eye( 4 ), df=5, size=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_and_grad( blah )( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t._value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad( blahh )( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsumexp( np.array( [ np.array( [ 1, 2, 3 ] ) ] ), axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blah():\n",
    "    data = 1\n",
    "    def __init__( self ):\n",
    "        self.data = blah.data\n",
    "        blah.data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blahs = [ blah() for i in range( 10 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( blahs )[ [ 1, 2, 3 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = np.array( blahs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle( asdf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list( range( 10 ) )[ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array( [-4.90028392, -0.54319243, -0.8875461 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[ p < 3 ] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah( x ):\n",
    "    return np.maximum( x, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random( 10 ) - 0.5\n",
    "jac( blah )( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random( ( 3, 4 ) )\n",
    "a = np.random.random( ( 5, 4 ) )\n",
    "b = np.random.random( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log( np.einsum( 'ij,tj->ti', w, a ) + b[ None ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = logsumexp( np.log( w[ None, :, : ] ) + np.log( a[ :, None, : ] ), axis=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log( np.exp( k - 4 ) + np.exp( np.log( b[ None ] ) - 4 ) ) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logadd( log_a, log_b ):\n",
    "    max_a = np.max( log_a )\n",
    "    max_b = np.max( log_b )\n",
    "    maximum = np.max( [ max_a, max_b ] )\n",
    "    return np.log( np.exp( log_a - maximum ) + np.exp( log_b - maximum ) ) + maximum\n",
    "import os\n",
    "import sys\n",
    "top_level_dir = '/'.join( os.getcwd().split( '/' )[ :-2 ] )\n",
    "if top_level_dir not in sys.path:\n",
    "    sys.path.append( top_level_dir )\n",
    "from GenModels.GM.Distributions.Normal import Normal\n",
    "from GenModels.GM.Distributions.TensorNormal import TensorNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array( [ [ 1 ] ] )\n",
    "d_out = 2\n",
    "d_in = 3\n",
    "recognizer_hidden_size = 10\n",
    "Wr1 = TensorNormal.generate( Ds=( recognizer_hidden_size, d_out ) )[ 0 ]\n",
    "br1 = Normal.generate( D=recognizer_hidden_size )\n",
    "Wr2 = TensorNormal.generate( Ds=( d_in, recognizer_hidden_size ) )[ 0 ]\n",
    "br2 = Normal.generate( D=d_in )\n",
    "recognizer_params = [ ( Wr1, br1 ), ( Wr2, br2 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn y into a one hot vector\n",
    "last_layer = np.zeros( ( y.shape[ 0 ], d_out ) )\n",
    "last_layer[ np.arange( y.shape[ 0 ] ), y ] = 1.0\n",
    "\n",
    "last_layer = np.log( last_layer )\n",
    "\n",
    "for W, b in recognizer_params[ :-1 ]:\n",
    "\n",
    "    # last_layer = np.tanh( np.einsum( 'ij,tj->ti', W, last_layer ) + b[ None ] )\n",
    "    k = logsumexp( np.log( W[ None, :, : ] ) + last_layer[ :, None, : ], axis=2 )\n",
    "    last_layer = logadd( k, np.log( b[ None ] ) )\n",
    "    last_layer = last_layer - logsumexp( last_layer, axis=1 )[ None ]\n",
    "\n",
    "W, b = recognizer_params[ -1 ]\n",
    "\n",
    "last_layer = logsumexp( np.log( W[ None, :, : ] ) + last_layer[ :, None, : ], axis=2 )\n",
    "last_layer = logsumexp( last_layer, axis=0 )\n",
    "last_layer = logadd( last_layer, np.log( b ) )\n",
    "# last_layer = np.einsum( 'ij,tj->i', W, last_layer ) + b\n",
    "logits = last_layer - logsumexp( last_layer )\n",
    "print( logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GenModel]",
   "language": "python",
   "name": "conda-env-GenModel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
